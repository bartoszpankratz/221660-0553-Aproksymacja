{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele generatywne; uczenie nienadzorowane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci kodujące-dekodujące"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Jak wiemy z poprzednich zajęć sieci neuronowe pozwalają nam efektywnie zredukować wymiar wejściowego zbioru danych, zazwyczaj w celu odpowiedniej klasyfikacji i predykcji na zadanym zbiorze. Co jednak jeśli jesteśmy zainteresowani tylko redukcją wymiaru danych a nie wnioskowaniem na ich podstawie?\n",
    "\n",
    "Klasyczna sieć kodująca-dekodująca (<b>autoencoder</b>) pozwala nam na zakodowanie danych (najczęściej obrazu) w formie wektora <b>zmiennych ukrytych</b> <i>(latent variables)</i> i następnie odtworzenie go w możliwie jak najbardziej bezstratnej formie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://kvfrans.com/content/images/2016/08/autoenc.jpg)](http://kvfrans.com/variational-autoencoders-explained/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tego typu sieci posiadają kilka zastosowań:\n",
    "\n",
    "- Są efektywnym narzędziem kompresji.\n",
    "- Można je stosować jako sposób szyfrowania danych.\n",
    "- Pozwalają na wykrycie nietrywialnych zależności między danymi, które nie są możliwe do uzyskania za pomocą klasycznych algorytmów uczenia nienadzorowanego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokażmy prosty przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message)\n",
      "└ @ CUDAdrv C:\\Users\\p\\.julia\\packages\\CUDAdrv\\mCr0O\\src\\CUDAdrv.jl:69\n"
     ]
    }
   ],
   "source": [
    "using Flux, Flux.Data.MNIST\n",
    "using Flux: mse, throttle, params, Statistics, @epochs\n",
    "using Base.Iterators: partition\n",
    "using Printf, BSON, LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = float.(hcat(vec.(MNIST.images())...))\n",
    "N, M = size(X, 2), 100\n",
    "data = [X[:,i] for i in Iterators.partition(1:N,M)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(32, 256, leakyrelu), Dense(256, 784, leakyrelu))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Chain(Dense(28^2, 256, leakyrelu),\n",
    "                Dense(256, 32, leakyrelu))\n",
    "    \n",
    "decoder = Chain(Dense(32, 256, leakyrelu), \n",
    "                Dense(256, 28^2, leakyrelu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.001, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(encoder, decoder)\n",
    "loss(x) = mse(model(x), x)\n",
    "evalcb = throttle(() -> @show(loss(data[1])), 60)\n",
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[11]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(data[1]) = 0.08695227443038495\n",
      "loss(data[1]) = 0.05522200830117088\n",
      "loss(data[1]) = 0.04271101870730842\n",
      "loss(data[1]) = 0.03428420629914682\n",
      "loss(data[1]) = 0.029653196647895563\n",
      "loss(data[1]) = 0.026133213585833717\n",
      "loss(data[1]) = 0.024004821294478503\n",
      "loss(data[1]) = 0.0223705859977821\n",
      "loss(data[1]) = 0.020996626711766702\n",
      "loss(data[1]) = 0.01999302743946103\n",
      "loss(data[1]) = 0.018960194715436956\n",
      "loss(data[1]) = 0.018018079805186275\n",
      "loss(data[1]) = 0.017086494928904718\n",
      "loss(data[1]) = 0.017035316213796427\n",
      "loss(data[1]) = 0.01656064963832795\n",
      "loss(data[1]) = 0.01614566077469304\n",
      "loss(data[1]) = 0.016111458475770273\n",
      "loss(data[1]) = 0.015528620948371489\n",
      "loss(data[1]) = 0.0151089517212893\n",
      "loss(data[1]) = 0.01482919676125699\n",
      "loss(data[1]) = 0.015002209191501832\n",
      "loss(data[1]) = 0.01467366485890775\n",
      "loss(data[1]) = 0.014209979337516498\n",
      "loss(data[1]) = 0.014333477583144358\n",
      "loss(data[1]) = 0.014066617111962214\n",
      "loss(data[1]) = 0.013826348484915199\n",
      "loss(data[1]) = 0.013686757888532573\n",
      "loss(data[1]) = 0.013568470427321245\n",
      "loss(data[1]) = 0.013328356312882643\n",
      "loss(data[1]) = 0.013289177158694826\n",
      "loss(data[1]) = 0.013453306771139574\n",
      "loss(data[1]) = 0.013195867033083347\n",
      "loss(data[1]) = 0.013200941048116544\n",
      "loss(data[1]) = 0.01284997480393462\n",
      "loss(data[1]) = 0.012958064652624685\n",
      "loss(data[1]) = 0.01263514553326108\n",
      "loss(data[1]) = 0.012812212863745119\n",
      "loss(data[1]) = 0.012547239582294074\n",
      "loss(data[1]) = 0.012260152750934934\n",
      "loss(data[1]) = 0.011991938025749571\n",
      "loss(data[1]) = 0.012116464517126247\n",
      "loss(data[1]) = 0.012220910674101679\n",
      "loss(data[1]) = 0.012071471695216579\n",
      "loss(data[1]) = 0.012651231609819255\n",
      "loss(data[1]) = 0.01207670654845317\n",
      "loss(data[1]) = 0.012200300462411044\n",
      "loss(data[1]) = 0.012271852474954117\n",
      "loss(data[1]) = 0.011811076145352681\n",
      "loss(data[1]) = 0.011751418755464831\n",
      "loss(data[1]) = 0.0118948317598396\n",
      "loss(data[1]) = 0.011786606211901035\n",
      "loss(data[1]) = 0.0117591574815512\n",
      "loss(data[1]) = 0.011515627240679801\n",
      "loss(data[1]) = 0.011447868621666857\n",
      "loss(data[1]) = 0.011575558861227244\n",
      "loss(data[1]) = 0.011460499719435004\n",
      "loss(data[1]) = 0.01137139939613802\n",
      "loss(data[1]) = 0.011410522785318901\n",
      "loss(data[1]) = 0.011226322556746663\n",
      "loss(data[1]) = 0.011613506158208855\n",
      "loss(data[1]) = 0.011366113184076787\n",
      "loss(data[1]) = 0.011351343355479378\n",
      "loss(data[1]) = 0.011401120143421718\n",
      "loss(data[1]) = 0.011058379462165391\n",
      "loss(data[1]) = 0.010964571808275592\n",
      "loss(data[1]) = 0.011001274784162966\n",
      "loss(data[1]) = 0.011151006256444076\n",
      "loss(data[1]) = 0.010995709012584459\n",
      "loss(data[1]) = 0.0110752643668683\n",
      "loss(data[1]) = 0.011068459914543007\n",
      "loss(data[1]) = 0.010936617928958565\n",
      "loss(data[1]) = 0.010938461663938538\n",
      "loss(data[1]) = 0.012507758179508525\n",
      "loss(data[1]) = 0.010673987659394578\n",
      "loss(data[1]) = 0.010493716959197893\n",
      "loss(data[1]) = 0.010407643931244938\n",
      "loss(data[1]) = 0.010535376361740114\n",
      "loss(data[1]) = 0.010382776134717123\n",
      "loss(data[1]) = 0.010823432240280892\n",
      "loss(data[1]) = 0.010427398081768152\n",
      "loss(data[1]) = 0.010420832722856998\n",
      "loss(data[1]) = 0.010712956132765473\n",
      "loss(data[1]) = 0.010438235035811472\n",
      "loss(data[1]) = 0.010284771623882189\n",
      "loss(data[1]) = 0.010294703412312851\n",
      "loss(data[1]) = 0.010323449488040324\n",
      "loss(data[1]) = 0.010239462671546514\n",
      "loss(data[1]) = 0.010060790101441112\n",
      "loss(data[1]) = 0.010190337895478815\n",
      "loss(data[1]) = 0.010339259583040144\n",
      "loss(data[1]) = 0.010283836496656394\n",
      "loss(data[1]) = 0.010115994925822807\n",
      "loss(data[1]) = 0.010290184203291924\n",
      "loss(data[1]) = 0.009982006914274083\n",
      "loss(data[1]) = 0.010197440790132451\n",
      "loss(data[1]) = 0.010112902849845581\n",
      "loss(data[1]) = 0.009973103286072912\n",
      "loss(data[1]) = 0.00990460313933475\n",
      "loss(data[1]) = 0.00994227011347311\n",
      "loss(data[1]) = 0.00973359045637413\n",
      "loss(data[1]) = 0.00998360629089498\n",
      "loss(data[1]) = 0.010011678552789316\n",
      "loss(data[1]) = 0.009861588519246869\n",
      "loss(data[1]) = 0.009896762444963786\n",
      "loss(data[1]) = 0.00987418814652935\n",
      "loss(data[1]) = 0.009809267949898916\n",
      "loss(data[1]) = 0.009803551213008868\n",
      "loss(data[1]) = 0.009636360054791593\n",
      "loss(data[1]) = 0.010211795491239864\n",
      "loss(data[1]) = 0.009411361355761434\n",
      "loss(data[1]) = 0.0093392321914031\n",
      "loss(data[1]) = 0.009379282635802733\n",
      "loss(data[1]) = 0.00949134402348009\n",
      "loss(data[1]) = 0.009319136527428938\n",
      "loss(data[1]) = 0.009766144180448251\n",
      "loss(data[1]) = 0.009427239048627396\n",
      "loss(data[1]) = 0.009476285412874373\n",
      "loss(data[1]) = 0.009683541649979298\n",
      "loss(data[1]) = 0.009366571213630524\n",
      "loss(data[1]) = 0.009292871039357966\n",
      "loss(data[1]) = 0.009360924560644584\n",
      "loss(data[1]) = 0.009265467371673393\n",
      "loss(data[1]) = 0.009221981305169753\n",
      "loss(data[1]) = 0.009177942805992288\n",
      "loss(data[1]) = 0.009340177879592812\n",
      "loss(data[1]) = 0.00930116765334996\n",
      "loss(data[1]) = 0.009309587374155489\n",
      "loss(data[1]) = 0.009458111982870774\n",
      "loss(data[1]) = 0.009405196692518167\n",
      "loss(data[1]) = 0.00929787545562456\n",
      "loss(data[1]) = 0.009231133342247125\n",
      "loss(data[1]) = 0.009051803730402532\n",
      "loss(data[1]) = 0.009128001492310495\n",
      "loss(data[1]) = 0.009048583307738585\n",
      "loss(data[1]) = 0.00913186503394656\n",
      "loss(data[1]) = 0.009099970684460825\n",
      "loss(data[1]) = 0.00910162847103391\n",
      "loss(data[1]) = 0.008835468371196847\n",
      "loss(data[1]) = 0.008613671442699195\n",
      "loss(data[1]) = 0.008800629030620238\n",
      "loss(data[1]) = 0.008814105506371442\n",
      "loss(data[1]) = 0.009024328866024639\n",
      "loss(data[1]) = 0.00870427901063895\n",
      "loss(data[1]) = 0.00860867443574997\n",
      "loss(data[1]) = 0.008713077559569422\n",
      "loss(data[1]) = 0.00870297457970683\n",
      "loss(data[1]) = 0.008864309379803379\n",
      "loss(data[1]) = 0.008705523453630615\n",
      "loss(data[1]) = 0.008752425473940244\n",
      "loss(data[1]) = 0.008569384939806387\n",
      "loss(data[1]) = 0.008632640661944933\n",
      "loss(data[1]) = 0.008568963121618764\n",
      "loss(data[1]) = 0.008737930433848944\n",
      "loss(data[1]) = 0.008640295021428532\n",
      "loss(data[1]) = 0.008675855683527007\n",
      "loss(data[1]) = 0.00854986640851324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning:  -> We're calling this converged.\n",
      "└ @ Main In[11]:15\n"
     ]
    }
   ],
   "source": [
    "@info(\"Beginning training loop...\")\n",
    "best_ls = Inf\n",
    "last_improvement = 0\n",
    "for epoch = 1:10\n",
    "    @info \"Epoch $i\"\n",
    "    global best_acc, last_improvement\n",
    "    Flux.train!(loss, params(model), zip(data), opt, cb = evalcb)\n",
    "    ls = loss(data[1])\n",
    "    if ls <= best_ls\n",
    "        BSON.@save \"enc_dec.bson\" model epoch\n",
    "        best_ls = ls\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 2\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package ImageMagick not found in current path:\n- Run `import Pkg; Pkg.add(\"ImageMagick\")` to install the ImageMagick package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package ImageMagick not found in current path:\n- Run `import Pkg; Pkg.add(\"ImageMagick\")` to install the ImageMagick package.\n",
      "",
      "Stacktrace:",
      " [1] require(::Module, ::Symbol) at .\\loading.jl:887",
      " [2] top-level scope at In[43]:1"
     ]
    }
   ],
   "source": [
    "using Images, ImageMagick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img(x::Vector) = Gray.(reshape(clamp.(x, 0, 1), 28, 28))\n",
    "\n",
    "function sample()\n",
    "  before = [MNIST.images()[i] for i in rand(1:length(MNIST.images()), 4)]\n",
    "  after = img.(map(x -> model(float(vec(x))), before))\n",
    "  vcat(hcat.(before, after)...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: imshow not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: imshow not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[41]:1"
     ]
    }
   ],
   "source": [
    "sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taki model pozwala na całkiem efektywną kompresję wejściowych danych. Jednak co należy zrobić gdy chcielibyśmy generować nowe obrazy na podstawie danych wejściowych a nie jedynie je odtwarzać?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wariacyjne autoenkodery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Variational Autoencoder](https://arxiv.org/pdf/1312.6114.pdf) jest klasą modeli, która na to pozwala. Intuicja  za nimi stojąca jest prosta, sieć dekodująca zamiast odtwarzać obraz w skali 1:1 dodaje do nich pewną losowość, dzięki której wyjściowy obraz jest nieznacznie zmieniony w stosunku do obrazu wejściowego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otrzymanie tej losowości jest możliwe dzięki wygenerowaniu przez sieć kodującą nie tyle wektora zmiennych ukrytych, co wektora średnich i odchyleń standardowych zmiennych ukrytych z których następnie losowane są zmienne ukryte wykorzystane przez sieć dekodującą:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://kvfrans.com/content/images/2016/08/vae.jpg)](http://kvfrans.com/variational-autoencoders-explained/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Jak działa taka sieć?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zgodnie z <b>hipotezą rozmaitości</b> (<i>Manifold hypothesis</i>) o kształcie  zbiorów danych o wysokim wymiarze decyduje tak naprawdę niewielka liczba zmiennych:\n",
    "\n",
    "[![](https://www.researchgate.net/profile/Y_Bengio/publication/221700451/figure/fig1/AS:305523212734474@1449853818623/Example-of-a-simple-manifold-in-the-space-of-images-associated-with-a-rather-low-level.png)](https://www.researchgate.net/figure/Example-of-a-simple-manifold-in-the-space-of-images-associated-with-a-rather-low-level_fig1_221700451/)\n",
    "\n",
    "Oznacza to, że do generowania dużyh zbiorów danych $x$ potrzebujemy tak naprawdę wiedzy jedynie o zmiennych ukrytych $z$, które je generują:\n",
    "\n",
    "[![](https://i.stack.imgur.com/w0HP5.png)](https://stats.stackexchange.com/questions/271522/what-is-the-objective-of-a-variational-autoencoder-vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tym co chcemy znaleźć jest rozkład prawdopodobieństwa $p(x,z)$, który wyjaśnia to w jaki sposób zmienne ukryte kształtują dane:\n",
    "\n",
    "$$p(x,z) = p(x|z)p(z)$$\n",
    "\n",
    "Mamy odwrotną informację:\n",
    "\n",
    "$$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$$\n",
    "\n",
    "Aby dostać $p(x,z)$ musimy wyznaczyć $p(x) = \\int {p(x|z)p(z)}dz$\n",
    "\n",
    "Jak to zrobić? \n",
    "- bezpośrednio?\n",
    "- generując dane z rozkladu i uśredniając wyniki?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast tego mozemy wyaproksymowac $p(z|x)$ za pomocą przyjętego rozkladu $Q(z|x; \\theta)$, który należy do znanej nam rodziny rozkladów prawdopodobieństwa. Wtedy problem sprowadza się do policzenia::\n",
    "$$E_{z \\tilde{}  Q}[P(x | z)]$$ \n",
    "\n",
    "\n",
    "Jak jednak $Q$ ma sie do $p$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mozemy to zmierzyc za pomoca Dywergencji Kullbacka-Leiblera:\n",
    "\n",
    "$$d_{KL}(Q(z|x)||p(z  |  x)) = E_{Q}[\\log Q(z|x)] - E_{Q}[\\log p(x,z)] + \\log p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za zadanie mamy znaleźć takie $Q$, które zminimalizuje dywergencje \n",
    "\n",
    "\n",
    "Aby to zrobić zdefiniujmy <b>Evidence Lower BOund</b> (ELBO):\n",
    "\n",
    "$$ELBO(\\theta) = E_{Q}[\\log p(x,z)] -  E_{Q}[\\log Q(z|x)]$$\n",
    "\n",
    "Zauważmy, że:\n",
    "\n",
    "$$\\log p(x) = ELBO(\\theta) + d_{KL}(Q(z|x)||p(z  |  x))$$\n",
    "\n",
    "Na mocy nierówności Jensena Dywergencja Kullbacka-Leiblera jest zawsze nieujemna, co oznacza, że problem minimalizacji dywergencji sprowadza się do problemu maksymalizacji ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdźmy do implementacji modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = float.(hcat(vec.(MNIST.images())...)) .> 0.5\n",
    "N, M = size(X, 2), 100\n",
    "data = [X[:,i] for i in Iterators.partition(1:N,M)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdefiniujmy funkcje straty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using Distributions.params in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logpdf (generic function with 64 methods)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extend distributions slightly to have a numerically stable logpdf for `p` close to 1 or 0.\n",
    "using Distributions\n",
    "import Distributions: logpdf\n",
    "logpdf(b::Bernoulli, y::Bool) = y * log(b.p + eps(Float32)) + (1f0 - y) * log(1 - b.p + eps(Float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kl_q_p (generic function with 1 method)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KL-divergence between approximation posterior and N(0, 1) prior\n",
    "kl_q_p(μ, logσ) = 0.5f0 * sum(exp.(2f0 .* logσ) + μ.^2 .- 1f0 .+ logσ.^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logp_x_z (generic function with 1 method)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logp(x|z) - conditional probability of data given latents.\n",
    "logp_x_z(x, z) = sum(logpdf.(Bernoulli.(decoder(z)), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ELBO (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ELBO(X)\n",
    "    μ̂, logσ̂ = μ(encoder(X)), logσ(encoder(X))\n",
    "    logp_x_z(X, z.(μ̂, logσ̂)) - kl_q_p(μ̂, logσ̂) * 1 // M\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X) = -ELBO(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dz, Dh = 2, 512 #wymiary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(784, 512, relu))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder  = Chain(Dense(28^2, 512, relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(512, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "μ = Dense(Dh, Dz)\n",
    "logσ = Dense(Dh, Dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "z (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z(μ, logσ) = μ + exp(logσ) * randn(Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(2, 512, relu), Dense(512, 784, σ))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Chain(Dense(Dz, Dh, relu), \n",
    "                Dense(Dh, 28^2, σ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalcb = throttle(() -> @show(loss(X[:, rand(1:N, M)])), 60)\n",
    "opt = ADAM()\n",
    "ps = params(encoder, μ, logσ, decoder);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @info(\"Beginning training loop...\")\n",
    "best_ls = Inf\n",
    "last_improvement = 0\n",
    "for epoch = 1:10\n",
    "    @info \"Epoch $epoch\"\n",
    "    global best_acc, last_improvement\n",
    "    Flux.train!(loss, ps, zip(data), opt, cb=evalcb)\n",
    "    ls = loss(data[1])\n",
    "    if ls <= best_ls\n",
    "        BSON.@save \"vae.bson\" encoder μ logσ decoder\n",
    "        best_ls = ls\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 2\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do budowania modeli generatywnych możemy wykorzystać też sieci konwolucyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dz, Dh = 6, 128 #wymiary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder =  Chain(x -> reshape(x, (28,28,1,100)),\n",
    "        Conv((3,3), 1=>32, pad = (1,1),relu),\n",
    "        Conv((3,3), 32=>32, pad = (1,1), relu),\n",
    "        MaxPool((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        x -> reshape(x, :, size(x, 4)),\n",
    "        Dense(6272, Dh))\n",
    "\n",
    "μ = Dense(Dh, Dz) \n",
    "\n",
    "logσ = Dense(Dh, Dz)\n",
    "\n",
    "z(μ, logσ) = μ + exp.(logσ) * randn(Float32)\n",
    "\n",
    "decoder = Chain(Dense(Dz, Dh, relu),\n",
    "        Dense(Dh,6272, relu),\n",
    "        x -> reshape(x,(14,14,32,100)),\n",
    "        Dropout(0.25),\n",
    "        x -> repeat(x, inner = [2,2,1,1]),\n",
    "        ConvTranspose((3,3), 32=>32, pad = (1,1),relu),\n",
    "        ConvTranspose((3,3), 32=>1, pad = (1,1), σ),\n",
    "        x -> reshape(x, :, size(x, 4))\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalcb = throttle(() -> @show(loss(X[:, rand(1:N, M)])), 300)\n",
    "opt = ADAM()\n",
    "ps = params(encoder, μ, logσ, decoder);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@info(\"Beginning training loop...\")\n",
    "best_ls = Inf\n",
    "last_improvement = 0\n",
    "for epoch = 1:10\n",
    "    @info \"Epoch $epoch\"\n",
    "    global best_acc, last_improvement\n",
    "    Flux.train!(loss, ps, zip(data), opt, cb=evalcb)\n",
    "    ls = loss(data[1])\n",
    "    if ls <= best_ls\n",
    "        BSON.@save \"vaeCNN_MNIST.bson\" encoder μ logσ decoder\n",
    "        best_ls = ls\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 2\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BSON.@load \"vaeCNN_MNIST.bson\" encoder μ logσ decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img(x::Vector) = Gray.(reshape(clamp.(x, 0, 1), 28, 28))\n",
    "\n",
    "function sample()\n",
    "    batch = data[rand(1:length(data))]\n",
    "    latents = encoder(batch)\n",
    "    Z = z(μ(latents), logσ(latents))\n",
    "    decoded = decoder(Z)\n",
    "    idx = rand(1:size(batch,2),4)\n",
    "    before = [img(batch[:,i]) for i in idx]\n",
    "    after = [img(decoded[:,i]) for i in idx]\n",
    "    vcat(hcat.(before, after)...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
