{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele generatywne; uczenie nienadzorowane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci kodujące-dekodujące"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Jak wiemy z poprzednich zajęć sieci neuronowe pozwalają nam efektywnie zredukować wymiar wejściowego zbioru danych, zazwyczaj w celu odpowiedniej klasyfikacji i predykcji na zadanym zbiorze. Co jednak jeśli jesteśmy zainteresowani tylko redukcją wymiaru danych a nie wnioskowaniem na ich podstawie?\n",
    "\n",
    "Klasyczna sieć kodująca-dekodująca (<b>autoencoder</b>) pozwala nam na zakodowanie danych (najczęściej obrazu) w formie wektora <b>zmiennych ukrytych</b> <i>(latent variables)</i> i następnie odtworzenie go w możliwie jak najbardziej bezstratnej formie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://kvfrans.com/content/images/2016/08/autoenc.jpg)](http://kvfrans.com/variational-autoencoders-explained/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tego typu sieci posiadają kilka zastosowań:\n",
    "\n",
    "- Są efektywnym narzędziem kompresji.\n",
    "- Można je stosować jako sposób szyfrowania danych.\n",
    "- Pozwalają na wykrycie nietrywialnych zależności między danymi, które nie są możliwe do uzyskania za pomocą klasycznych algorytmów uczenia nienadzorowanego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokażmy prosty przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message)\n",
      "└ @ CUDAdrv C:\\Users\\p\\.julia\\packages\\CUDAdrv\\mCr0O\\src\\CUDAdrv.jl:69\n"
     ]
    }
   ],
   "source": [
    "using Flux, Flux.Data.MNIST\n",
    "using Flux: mse, throttle, params, Statistics, @epochs\n",
    "using Base.Iterators: partition\n",
    "using Printf, BSON, LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = float.(hcat(vec.(MNIST.images())...))\n",
    "N, M = size(X, 2), 100\n",
    "data = [X[:,i] for i in Iterators.partition(1:N,M)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(32, 256, leakyrelu), Dense(256, 784, leakyrelu))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Chain(Dense(28^2, 256, leakyrelu),\n",
    "                Dense(256, 32, leakyrelu))\n",
    "    \n",
    "decoder = Chain(Dense(32, 256, leakyrelu), \n",
    "                Dense(256, 28^2, leakyrelu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.001, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(encoder, decoder)\n",
    "loss(x) = mse(model(x), x)\n",
    "evalcb = throttle(() -> @show(loss(data[1])), 60)\n",
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@info(\"Beginning training loop...\")\n",
    "best_ls = Inf\n",
    "last_improvement = 0\n",
    "for epoch = 1:10\n",
    "    @info \"Epoch $i\"\n",
    "    global best_ls, last_improvement\n",
    "    Flux.train!(loss, params(model), zip(data), opt, cb = evalcb)\n",
    "    ls = loss(data[1])\n",
    "    if ls <= best_ls\n",
    "        BSON.@save \"enc_dec.bson\" model epoch\n",
    "        best_ls = ls\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 2\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BSON.@load \"enc_dec.bson\" model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img(x::Vector) = Gray.(reshape(clamp.(x, 0, 1), 28, 28))\n",
    "\n",
    "function sample()\n",
    "  before = [MNIST.images()[i] for i in rand(1:length(MNIST.images()), 4)]\n",
    "  after = img.(map(x -> model(float(vec(x))), before))\n",
    "  vcat(hcat.(before, after)...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32-element Array{Float32,1}:\n",
       " -0.0031461504 \n",
       "  0.14641653   \n",
       " -0.00053805136\n",
       " -0.0028649191 \n",
       " -0.010664173  \n",
       "  0.079312384  \n",
       " -0.0014963194 \n",
       "  0.007531166  \n",
       "  0.30741718   \n",
       "  0.17018029   \n",
       " -0.004722144  \n",
       "  0.28534555   \n",
       " -0.00289976   \n",
       "  ⋮            \n",
       "  0.15918723   \n",
       "  0.18245387   \n",
       "  0.10285814   \n",
       "  1.3044422    \n",
       "  0.67391187   \n",
       " -2.8246232e-5 \n",
       " -0.0024048304 \n",
       "  0.019954836  \n",
       " -0.00057598733\n",
       "  0.28967044   \n",
       " -2.0092464e-5 \n",
       " -0.0028463914 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(float(vec(MNIST.images()[1] )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taki model pozwala na całkiem efektywną kompresję wejściowych danych. Jednak co należy zrobić gdy chcielibyśmy generować nowe obrazy na podstawie danych wejściowych a nie jedynie je odtwarzać?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wariacyjne autoenkodery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Variational Autoencoder](https://arxiv.org/pdf/1312.6114.pdf) jest klasą modeli, która na to pozwala. Intuicja  za nimi stojąca jest prosta, sieć dekodująca zamiast odtwarzać obraz w skali 1:1 dodaje do nich pewną losowość, dzięki której wyjściowy obraz jest nieznacznie zmieniony w stosunku do obrazu wejściowego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otrzymanie tej losowości jest możliwe dzięki wygenerowaniu przez sieć kodującą nie tyle wektora zmiennych ukrytych, co wektora średnich i odchyleń standardowych zmiennych ukrytych z których następnie losowane są zmienne ukryte wykorzystane przez sieć dekodującą:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://kvfrans.com/content/images/2016/08/vae.jpg)](http://kvfrans.com/variational-autoencoders-explained/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Jak działa taka sieć?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zgodnie z <b>hipotezą rozmaitości</b> (<i>Manifold hypothesis</i>) o kształcie  zbiorów danych o wysokim wymiarze decyduje tak naprawdę niewielka liczba zmiennych:\n",
    "\n",
    "[![](https://www.researchgate.net/profile/Y_Bengio/publication/221700451/figure/fig1/AS:305523212734474@1449853818623/Example-of-a-simple-manifold-in-the-space-of-images-associated-with-a-rather-low-level.png)](https://www.researchgate.net/figure/Example-of-a-simple-manifold-in-the-space-of-images-associated-with-a-rather-low-level_fig1_221700451/)\n",
    "\n",
    "Oznacza to, że do generowania dużyh zbiorów danych $x$ potrzebujemy tak naprawdę wiedzy jedynie o zmiennych ukrytych $z$, które je generują:\n",
    "\n",
    "[![](https://i.stack.imgur.com/w0HP5.png)](https://stats.stackexchange.com/questions/271522/what-is-the-objective-of-a-variational-autoencoder-vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tym co chcemy znaleźć jest rozkład prawdopodobieństwa $p(x,z)$, który wyjaśnia to w jaki sposób zmienne ukryte kształtują dane:\n",
    "\n",
    "$$p(x,z) = p(x|z)p(z)$$\n",
    "\n",
    "Mamy odwrotną informację:\n",
    "\n",
    "$$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$$\n",
    "\n",
    "Aby dostać $p(x,z)$ musimy wyznaczyć $p(x) = \\int {p(x|z)p(z)}dz$\n",
    "\n",
    "Jak to zrobić? \n",
    "- bezpośrednio?\n",
    "- generując dane z rozkladu i uśredniając wyniki?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast tego mozemy wyaproksymowac $p(z|x)$ za pomocą przyjętego rozkladu $Q(z|x; \\theta)$, który należy do znanej nam rodziny rozkladów prawdopodobieństwa. Wtedy problem sprowadza się do policzenia::\n",
    "$$E_{z \\tilde{}  Q}[P(x | z)]$$ \n",
    "\n",
    "\n",
    "Jak jednak $Q$ ma sie do $p$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mozemy to zmierzyc za pomoca Dywergencji Kullbacka-Leiblera:\n",
    "\n",
    "$$d_{KL}(Q(z|x)||p(z  |  x)) = E_{Q}[\\log Q(z|x)] - E_{Q}[\\log p(x,z)] + \\log p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za zadanie mamy znaleźć takie $Q$, które zminimalizuje dywergencje \n",
    "\n",
    "\n",
    "Aby to zrobić zdefiniujmy <b>Evidence Lower BOund</b> (ELBO):\n",
    "\n",
    "$$ELBO(\\theta) = E_{Q}[\\log p(x,z)] -  E_{Q}[\\log Q(z|x)]$$\n",
    "\n",
    "Zauważmy, że:\n",
    "\n",
    "$$\\log p(x) = ELBO(\\theta) + d_{KL}(Q(z|x)||p(z  |  x))$$\n",
    "\n",
    "Na mocy nierówności Jensena Dywergencja Kullbacka-Leiblera jest zawsze nieujemna, co oznacza, że problem minimalizacji dywergencji sprowadza się do problemu maksymalizacji ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdźmy do implementacji modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = float.(hcat(vec.(MNIST.images())...)) .> 0.5\n",
    "N, M = size(X, 2), 100\n",
    "data = [X[:,i] for i in Iterators.partition(1:N,M)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwAQAAAADum8nmAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QAAd2KE6QAAABYSURBVDjL7dPLCcAwDANQb6D9t/QGSttL694kQaBQ3d4h/kGq/shBsy2CZyxeuKtuI8CCyWP3190U0mclzBq3T0xLjKYOCNoEfU5J9J8GA+P5sUVOafxcFpXv0NV438lcAAAAAElFTkSuQmCC",
      "text/plain": [
       "28×28 Array{Gray{Float64},2} with eltype Gray{Float64}:\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img(Float64.(X[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdefiniujmy funkcje straty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using Distributions.params in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logpdf (generic function with 64 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extend distributions slightly to have a numerically stable logpdf for `p` close to 1 or 0.\n",
    "using Distributions\n",
    "import Distributions: logpdf\n",
    "logpdf(b::Bernoulli, y::Bool) = y * log(b.p + eps(Float32)) + (1f0 - y) * log(1 - b.p + eps(Float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kl_q_p (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KL-divergence between approximation posterior and N(0, 1) prior\n",
    "kl_q_p(μ, logσ) = 0.5f0 * sum(exp.(2f0 .* logσ) + μ.^2 .- 1f0 .+ logσ.^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logp_x_z (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logp(x|z) - conditional probability of data given latents.\n",
    "logp_x_z(x, z) = sum(logpdf.(Bernoulli.(decoder(z)), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ELBO (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ELBO(X)\n",
    "    μ̂, logσ̂ = μ(encoder(X)), logσ(encoder(X))\n",
    "     kl_q_p(μ̂, logσ̂) * 1 // M\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X) = -ELBO(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dz, Dh = 2, 512 #wymiary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder  = Chain(Dense(28^2, 512, relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "μ = Dense(Dh, Dz)\n",
    "logσ = Dense(Dh, Dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z(μ, logσ) = μ + exp(logσ) * randn(Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder = Chain(Dense(Dz, Dh, relu), \n",
    "                Dense(Dh, 28^2, σ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evalcb = throttle(() -> @show(loss(X[:, rand(1:N, M)])), 60)\n",
    "opt = ADAM()\n",
    "ps = params(encoder, μ, logσ, decoder);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## @info(\"Beginning training loop...\")\n",
    "best_ls = Inf\n",
    "last_improvement = 0\n",
    "for epoch = 1:10\n",
    "    @info \"Epoch $epoch\"\n",
    "    global best_ls, last_improvement\n",
    "    Flux.train!(loss, ps, zip(data), opt, cb=evalcb)\n",
    "    ls = loss(data[1])\n",
    "    if ls <= best_ls\n",
    "        BSON.@save \"vae.bson\" encoder μ logσ decoder\n",
    "        best_ls = ls\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 2\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do budowania modeli generatywnych możemy wykorzystać też sieci konwolucyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 128)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dz, Dh = 6, 128 #wymiary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(6, 128, relu), Dense(128, 6272, relu), #23, Dropout(0.25), #24, ConvTranspose((3, 3), 32=>32, relu), ConvTranspose((3, 3), 32=>1, σ), #25)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder =  Chain(x -> reshape(x, (28,28,1,100)),\n",
    "        Conv((3,3), 1=>32, pad = (1,1),relu),\n",
    "        Conv((3,3), 32=>32, pad = (1,1), relu),\n",
    "        MaxPool((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        x -> reshape(x, :, size(x, 4)),\n",
    "        Dense(6272, Dh))\n",
    "\n",
    "μ = Dense(Dh, Dz) \n",
    "\n",
    "logσ = Dense(Dh, Dz)\n",
    "\n",
    "z(μ, logσ) = μ + exp.(logσ) * randn(Float32)\n",
    "\n",
    "decoder = Chain(Dense(Dz, Dh, relu),\n",
    "        Dense(Dh,6272, relu),\n",
    "        x -> reshape(x,(14,14,32,100)),\n",
    "        Dropout(0.25),\n",
    "        x -> repeat(x, inner = [2,2,1,1]),\n",
    "        ConvTranspose((3,3), 32=>32, pad = (1,1),relu),\n",
    "        ConvTranspose((3,3), 32=>1, pad = (1,1), σ),\n",
    "        x -> reshape(x, :, size(x, 4))\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evalcb = throttle(() -> @show(loss(X[:, rand(1:N, M)])), 300)\n",
    "opt = ADAM()\n",
    "ps = params(encoder, μ, logσ, decoder);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@info(\"Beginning training loop...\")\n",
    "best_ls = Inf\n",
    "last_improvement = 0\n",
    "for epoch = 1:10\n",
    "    @info \"Epoch $epoch\"\n",
    "    global best_ls, last_improvement\n",
    "    Flux.train!(loss, ps, zip(data), opt, cb=evalcb)\n",
    "    ls = loss(data[1])\n",
    "    if ls <= best_ls\n",
    "        BSON.@save \"vaeCNN_MNIST.bson\" encoder μ logσ decoder\n",
    "        best_ls = ls\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 2\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BSON.@load \"vaeCNN_MNIST.bson\" encoder μ logσ decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img(x::Vector) = Gray.(reshape(clamp.(x, 0, 1), 28, 28))\n",
    "\n",
    "function sample()\n",
    "    batch = data[rand(1:length(data))]\n",
    "    latents = encoder(batch)\n",
    "    Z = z(μ(latents), logσ(latents))\n",
    "    decoded = decoder(Z)\n",
    "    idx = rand(1:size(batch,2),4)\n",
    "    before = [img(Float64.(batch[:,i])) for i in idx]\n",
    "    after = [img(decoded[:,i]) for i in idx]\n",
    "    vcat(hcat.(before, after)...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "112×56 Array{Gray{Float64},2} with eltype Gray{Float64}:\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(1.28761e-10)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(1.24593e-13)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(3.42481e-12)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(1.27183e-9) \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(8.9621e-8)  \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(2.88123e-7) \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(7.63456e-7) \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(2.0037e-6)  \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(3.01443e-6) \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(4.19314e-6) \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(1.23322e-5) \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(2.05132e-5) \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(1.56224e-5) \n",
       " ⋮                                       ⋱  ⋮                         \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.000280788)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.000391066)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.000441843)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.000584614)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.000260615)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(1.54579e-5) \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(4.4058e-8)  \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(3.17643e-10)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(3.59349e-12)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(8.75475e-14)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(2.62595e-14)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(6.03179e-12)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dodatkowa praca domowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podstawie prezentowanego na zajęciach przykładu wyucz wariacyjny autoenkoder w taki sposób żeby reprezentował dane za pomocą wektora dwóch zmiennych ukrytych z możliwie najwyższą dokładnością. Następnie:\n",
    "1. Przedstaw na wykresie otrzymany wynik. Jak zachowują się zmienne ukryte?\n",
    "2. Wyucz sieć o takiej samej architekturze korzystając jedynie z warunkowego prawdopodobieństwa $logp(x|z)$ jako funkcji straty. Czym różni się otrzymany wynik? \n",
    "3. Wyucz taką samą sieć korzystając jedynie z Dywergencji Kullbacka-Leiblera jako funkcji straty. Ponownie porównaj wynik z poprzednio otrzymanymi."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
