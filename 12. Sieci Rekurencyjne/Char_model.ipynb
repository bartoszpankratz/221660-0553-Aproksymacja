{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wpowadzenie do deep learning w bibliotece Flux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby w  zrozumieć sposób pracy z Fluxem warto rozpatrzeć prosty przykład. Zajmiemy się przetwarzaniem języka naturalnego - zbudujemy model zdolny do generowania składnej wypowiedzi w języku polskim.\n",
    "\n",
    "Wyjściowe założenie jest takie, że wytrenujemy sieć neuronową, która będzie estymowała prawdopodobieństwo wystąpienia danego znaku w ciągu na podstawie poprzedzających go znaków w sekwencji ([<b>Character-Level Language Model</b>](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbiórem na którym będziemy pracowali jest <i>W poszukiwaniu straconego czasu</i> Marcela Prousta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://upload.wikimedia.org/wikipedia/commons/b/b8/Marcel_Proust_1895.jpg)](https://pl.wikipedia.org/wiki/Marcel_Proust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">(...) matka widząc, że mi jest zimno, namówiła mnie, abym się napił wbrew zwyczajowi trochę herbaty. Odmówiłem zrazu; potem, nie wiem czemu, namyśliłem się. Posłała po owe krótkie i pulchne ciasteczka zwane magdalenkami, które wyglądają jak odlane w prążkowanej skorupie muszli. I niebawem (...) machinalnie podniosłem do ust łyżeczkę herbaty, w której rozmoczyłem kawałek magdalenki. Ale w tej samej chwili, kiedy łyk pomieszany z okruchami ciasta dotknął mego podniebienia, zadrżałem, czując, że się we mnie dzieje coś niezwykłego. Owładnęła mną rozkoszna słodycz (...). Sprawiła, że w jednej chwili koleje życia stały mi się obojętne, klęski jako błahe, krótkość złudna (...). Cofam się myślą do chwili, w której wypiłem pierwszą łyżeczkę herbaty (...). I nagle wspomnienie zjawiło mi się. Ten smak to była magdalenka cioci Leonii.(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zanim jednak zaczniemy wprowadźmy odrobinę teorii stojącej za tym zagadnieniem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rekurencyjne sieci neuronowe (Recurrent neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Charakterystyczną cechą tego typu sieci jest to, że pozwalają one na istnienie wewnątrz grafu cykli skierowanych.\n",
    "- Oznacza to, że informacja wewnątrz takiej sieci nie musi płynąć tylko w jednym kierunku - neurony leżące na tej samej warstwie także mogą przesyłać sobie wzajemnie dane:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://karpathy.github.io/assets/rnn/diags.jpeg)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dzięki tej właściwości RNN doskonale nadają się do budowy interesującego nas modelu: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://karpathy.github.io/assets/rnn/charseq.jpeg)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Long short-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Problemem na który można natrafić w przypadku korzystania z RNN jest pamięć takiej sieci. Gdy odległość pomiędzy aktualnym a poprzedzającymi go węzłami, które niosą za sobą kluczową informację jest niewielka, sieć jest w stanie efektywnie je wykorzystać:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Problem się pojawia gdy ta odległość jest duża - wtedy kluczowe informacje po prostu znikają w szumie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wtedy też, warto zastosować sieć LSTM, która ze względu na swoją architekturę jest w stanie odpowiednio filtrować informację i wykorzystawać je nawet wtedy, gdy ich źródło jest znacznie oddalone od aktualnego neuronu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatywy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast bazowych sieci rekurencyjnych lub sieci LSTM (i ich [modyfikacji](https://en.wikipedia.org/wiki/Long_short-term_memory)) można zastosować różne alternatywy, np. sieci <b>Gated Recurrent Unit<b> (GRU):\n",
    "    \n",
    "[![](https://upload.wikimedia.org/wikipedia/commons/5/5f/Gated_Recurrent_Unit.svg)](https://en.wikipedia.org/wiki/Gated_recurrent_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lub inne modele skonstruowane do rozwiązywania specyficznych problemów, np. [uczenia na szeregach czasowych.](https://github.com/sdobber/FluxArchitectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Przejdźmy teraz do implementowania modelu za pomocą Fluxa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: onehot, onehotbatch, argmax, chunk, batchseq, crossentropy\n",
    "using StatsBase: wsample\n",
    "using Base.Iterators: partition\n",
    "using BSON\n",
    "using JLD2\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining on GPU\n"
     ]
    }
   ],
   "source": [
    " if use_cuda && CUDA.functional()\n",
    "    device = gpu\n",
    "    @info \"Training on GPU\"\n",
    "else\n",
    "    device = cpu\n",
    "    @info \"Training on CPU\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem jest oczywiście odpowiednie przygotowanie danych na których będziemy pracowali:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isfile(\"w_poszukiwaniu.txt\") ||\n",
    "        download(\"https://raw.githubusercontent.com/bartoszpankratz/221660-0553-Aproksymacja/master/6.%20Sieci%20Rekurencyjne/w_poszukiwaniu.txt\",\"w_poszukiwaniu.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = collect(read(\"w_poszukiwaniu.txt\",String));\n",
    "alphabet = [unique(text)..., '_'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie kodujemy zmienne jakościowe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = length(alphabet);\n",
    "seqlen = 100;\n",
    "batch_size = 32;\n",
    "stop = '_';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = collect(partition(Flux.onehotbatch.(batchseq(chunk(text, batch_size), stop), (alphabet,)), seqlen)) |> device;\n",
    "Ys = collect(partition(Flux.onehotbatch.(batchseq(chunk(text[2:end], batch_size), stop), (alphabet,)), seqlen)) |> device;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(\n",
    "    LSTM(N, 128),\n",
    "    LSTM(128, 512),\n",
    "    LSTM(512, 256),\n",
    "    Dense(256, 128, relu),\n",
    "    Dense(128, 64, relu),\n",
    "    Dense(64, N),\n",
    "    softmax) |> device\n",
    "\n",
    "function loss(model, xs, ys, ϵ = 1.0f-32)\n",
    "  l = sum(crossentropy.(broadcast(x -> model(x) .+ ϵ, xs), ys))\n",
    "  Flux.reset!(m)\n",
    "  return l\n",
    "end\n",
    "\n",
    "opt = ADAM(0.001)\n",
    "opt_state = Flux.setup(opt, m);\n",
    "\n",
    "function sample(m, alphabet, len; temp = 1)\n",
    "    model = cpu(m)\n",
    "    Flux.reset!(model)\n",
    "    buf = IOBuffer()\n",
    "    c = rand(alphabet)\n",
    "    for i = 1:len\n",
    "        write(buf, c)\n",
    "        c = wsample(alphabet, model(onehot(c, alphabet)))\n",
    "      end\n",
    "    return String(take!(buf))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481.22394f0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(m, Xs[5], Ys[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Żż*)\\uadźîSĘłćbioüąBèóàséróńużń,äB!.Jś\\uadїWIáR—2»ÀDZŻAF\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(m, alphabet, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mBeginning training loop...\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 1\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m4½½àüWwwôyéöùc;﻿SNenÀ'8”9ĆÉrèMkr6w)àttÊęp?'ëàĘ﻿îS7f„ŃäŚ0dQÓüÊSpŹîŻńYżp1ź.KÊi½Ń-łóĆlXfÉ8sëKŁçbm)â_Y…6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 274.72607f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mNew best result: 274.72607\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 2\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mH1*2jókoiiiuięta namierwi, miałniegrył X lu jarczy-Czak: I, trzy bład Myźć Albe onanie taczym rum, t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 286.4451f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 3\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mxyizrZrhawy k ałyjdłcage ł uncam słio AiGo wylol,a nioma tośofzelwihsć dczabetectełzzo zoulłaśtte s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 292.92218f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 4\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m( ć Ninyłienatąłaogugysziażorhwawi wiziaśraga onineremo kawifezichbyjowiótłio Maznięrolizanięrważrhn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 332.07285f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 5\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39máąe  —sć—wCKjgprybci re tąm y ki szkłnyją, pdbaaużejuwazaTl prod pt b, m grz miś rowy wezapizik je z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 287.02463f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 6\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m2r   a h n Veektgmne maśpmak re zid pałctw n pr re de uw ot wały premra derymeż z, j ej gakt Kne. Ci\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 337.02335f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 7\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mF\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39mmmA”wrczż ca— Itczłci chm cz Crł Scz, wcz ny) drjczos blk, so rs woru j p sw r, dłm, wro ć Gnn ci\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 293.88785f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 8\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39méb ——T—„Ogjnarwrnorz MyV, ny wie Schobiniw, Nys m ś.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39mPgrigrt poarjóktw czakś j k pt Michstów dz. ż \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 304.41364f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 9\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mùhkpLzztuopowaezzajliąńchjłórzonajepamnzyzekjch z popł APczwz ozzzicj zciesw z p ośrj żej, zizzwż Cu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 387.53015f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 10\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mANW\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mNLDBMrpcczbć sw. Nw” — Z cz  p*kwsuwsśczczż A\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39mPbeA Sb, mBsp st J — Wnż stć Prczdzwś cz m*w  sd N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 298.232f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch: 11\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meU\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39mAWpopówanoskcowd nzltll nidrad strgł, R sksśwzstustaczaw w pkssztrzać A zdbodza  gk.;coca  bodpoZ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls = 289.48996f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m -> We're calling this converged.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main In[11]:20\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@info(\"Beginning training loop...\")\n",
    "best_ls = Inf\n",
    "last_improvement = 0\n",
    "for epoch = 1:25\n",
    "    @info \"Epoch: $epoch\"\n",
    "    global best_ls, last_improvement\n",
    "    @info sample(m, alphabet, 100)\n",
    "    Flux.train!(loss, m, zip(Xs, Ys), opt_state)\n",
    "    ls = loss(m, Xs[5], Ys[5])\n",
    "    @show ls\n",
    "    if ls <= best_ls      \n",
    "        @info \"New best result: $ls\"\n",
    "        char_model = cpu(Flux.state(m)) \n",
    "        BSON.@save \"char_model.bson\" char_model\n",
    "        jldsave(\"char_model.jld2\"; char_model)\n",
    "        best_ls = ls\n",
    "        last_improvement = epoch\n",
    "    end\n",
    "    if epoch - last_improvement >= 10\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m = Chain(\n",
    "    LSTM(N, 128),\n",
    "    LSTM(128, 512),\n",
    "    LSTM(512, 256),\n",
    "    Dense(256, 128, relu),\n",
    "    Dense(128, 64, relu),\n",
    "    Dense(64, N),\n",
    "    softmax) |> device\n",
    "\n",
    "char_model = params(m)\n",
    "\n",
    "BSON.@load \"char_model.bson\" char_model\n",
    "\n",
    "Flux.loadparams!(m, device.(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "    LSTM(N, 128),\n",
    "    LSTM(128, 512),\n",
    "    LSTM(512, 256),\n",
    "    Dense(256, 128, relu),\n",
    "    Dense(128, 64, relu),\n",
    "    Dense(64, N),\n",
    "    softmax) |> device\n",
    "\n",
    "ps = JLD2.load(\"char_model.jld2\", \"char_model\")\n",
    "\n",
    "Flux.loadparams!(m, device.(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show loss(Xs[5], Ys[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(m, alphabet, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodatkowa praca domowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmodyfikuj kod tak, aby poprawić jakość generowanego tekstu <b>(8 punktów)</b>."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
